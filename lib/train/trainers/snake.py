import torch.nn as nn
from lib.utils import net_utils
import torch
from torch.nn import functional as F
from lib.utils.snake import snake_config, snake_gcn_utils
from lib.utils.snake.snake_gcn_utils import paste_masks_in_image, get_gcn_feature
from lib.csrc.roi_align_layer.roi_align import ROIAlign
from lib.config import cfg
#import sys 
#sys.path.insert(0, sys.path[0]+"/../../snake/")
from lib.networks.snake.mask_encoding import DctMaskEncoding
class NetworkWrapper(nn.Module):
    def __init__(self, net,isTrain=True):
        super(NetworkWrapper, self).__init__()

        self.net = net
        self.isTrain=isTrain
        self.m_crit = net_utils.FocalLoss() ## mask loss
        self.ct_crit = net_utils.FocalLoss() ## center loss
        self.cls_crit = net_utils.ClsCrossEntropyLoss()
        self.py_crit = torch.nn.functional.smooth_l1_loss  ## poly loss
        self.mask_size_assemble = cfg.roi_h*4
        self.scale = 14 
        self.patch_size = 8 # 这两个相乘要等于mask_size_assemble
        self.dct_vector_dim = 300
        self.patch_dct_vector_dim = 6
        self.mask_size = cfg.roi_h*4
        self.gt_pooler = ROIAlign((cfg.roi_h*4, cfg.roi_w*4)) # => (112,112) #TODO 后面写到配置文件中
        self.vis_gt_pooler = ROIAlign((cfg.roi_h, cfg.roi_w))
        self.pool = nn.AvgPool2d(kernel_size=2) #对1/2大小特征图的hm进行处理
        self.dct_encoding = DctMaskEncoding(vec_dim=self.dct_vector_dim, mask_size=self.mask_size)
        self.patch_dct_encoding = DctMaskEncoding(vec_dim=self.patch_dct_vector_dim, mask_size=self.patch_size)
        self.dct_stage_loss_para = [0.2,0.3,0.4,0.5,0.5,0.6,0.7,0.8]
        
    def shape_loss(self, pred, targ_shape):
        pre_dis = torch.cat((pred[:,1:], pred[:,0].unsqueeze(1)), dim=1)
        pred_shape = pre_dis-pred
        # targ_shape = targ(:,:,1:)-targ(:,:,:-1)
        loss = self.py_crit(pred_shape, targ_shape)
        return loss

    def postprocess(self, pred_masks, output_height, output_width):
        for i in range(len(pred_masks)):
            # pred_masks[i] = pred_masks[i].expand(1,-1,-1,-1)
            pred_masks[i] = F.interpolate(pred_masks[i],size=(output_height, output_width),mode="bilinear", align_corners=False)
        return pred_masks
    
    def crop_and_resize(self, gt_masks, rois):
        device = gt_masks.device
        batch_inds = torch.arange(len(rois), device=device).to(dtype=rois.dtype)[:, None]
        rois = torch.cat([batch_inds, rois[:,1:]], dim=1)  # Nx5
        output = self.gt_pooler(gt_masks[:,None,:,:], rois).squeeze(1)
        output = output >= 0.5
        return output
    
    def crop_and_resize2(self, gt_masks, rois, mask_sizes):
        #将gt_masks按预测的box裁剪后，使用roi align变为mask_sizes
        device = gt_masks.device
        batch_inds = torch.arange(len(rois), device=device).to(dtype=rois.dtype)[:, None]
        rois = torch.cat([batch_inds, rois[:,1:]], dim=1)  # Nx5
        align_class = ROIAlign((mask_sizes, mask_sizes))
        output = align_class(gt_masks[:,None,:,:], rois).squeeze(1)
        output = output >= 0.5
        return output
    
    def vis_crop_and_resize(self, gt_masks, rois):
        device = gt_masks.device
        batch_inds = torch.arange(len(rois), device=device).to(dtype=rois.dtype)[:, None]
        rois = torch.cat([batch_inds, rois[:,1:]], dim=1)  # Nx5
        output = self.vis_gt_pooler(gt_masks[:,None,:,:], rois).squeeze(1)
        output = output >= 0.5
        return output
    
    def masks2patch(self,masks_per_image, scale, patch_size, mask_size):
        """

        Args:
            masks_per_image: A tensor of shape [B,mask_size,mask_size]
            scale: A NxN mask size is divided into scale x scale patches
            patch_size: size of each patch
            mask_size: size of masks generated by PatchDCT

        Returns:
            patches_per_image: A tensor of shape [B*num_patch,patch_size,patch_size]. The patches obtained by masks

        """
        masks_per_image = masks_per_image.reshape(-1, scale, patch_size,mask_size)
        masks_per_image = masks_per_image.permute(0, 1, 3, 2) #n 14 8 112 => n 14 112 8 
        masks_per_image = masks_per_image.reshape(-1, scale, scale, patch_size, patch_size) #n 14 112 8 => n 14 14 8 8
        masks_per_image = masks_per_image.permute(0, 1, 2, 4, 3) #第一个14对应第一个8 因此调位
        patches_per_image = masks_per_image.reshape(-1,patch_size, patch_size) #reshape 为 n*196 8 8
        return patches_per_image
    
    def get_gt_mask(self,per_ins_cmask , rois):
        #针对单个实例预测 在ICD中，
        #gt_masks = []
        gt_classes = []
        gt_masks_coarse = []
        
        # for i in range(len(output['mask_preds'])):
        #     pred_masks = output['mask_preds'][i]
        #pred_masks = pred_masks[torch.arange(pred_masks.shape[0]),batch['ct_cls'][batch['ct_01'].byte()]]
        gt_masks_coarse = self.crop_and_resize2(per_ins_cmask, rois, self.mask_size) # 获得对应rois的gt mask，resize为mask_size
        
        gt_masks = self.crop_and_resize2(per_ins_cmask, rois, self.mask_size_assemble) #【n，mask_size_assemble，mask_size_assemble】
        gt_masks = self.masks2patch(gt_masks,self.scale,self.patch_size,self.mask_size_assemble) # [B*num_patch,patch_size,patch_size] num_patch即分成了多少个patch
        
        gt_masks = self.patch_dct_encoding.encode(gt_masks) # 将gt_masks encode为 dct编码的 [3332 6]
        gt_masks = gt_masks.to(dtype=torch.float32) #[N_instance,pdct_vector_dim]
        
        gt_masks_coarse = self.dct_encoding.encode(gt_masks_coarse).to(dtype=torch.float32) # gt_masks_coarse也要进行encoding
        gt_masks, gt_bfg = self.get_gt_bfg(gt_masks) #gt_bfg是将dct_encoding后的bfg进行处理，0是前景 1是边缘 2是背景 

        return gt_masks,gt_masks_coarse,gt_bfg
    
    def get_gt_bfg(self, gt_masks):
        gt_bfg = gt_masks[:, 0].clone()
        gt_bfg[(gt_bfg > 0) & (gt_bfg < self.patch_size)] = 1.
        gt_bfg[gt_bfg == self.patch_size] = 2
        gt_bfg = gt_bfg.to(dtype=torch.int64)
        gt_masks = gt_masks[gt_bfg == 1, :]
        return gt_masks, gt_bfg
    
    def forward(self, batch):
        output = self.net(batch['inp'], batch)
        #ct_01=batch['ct_01'].byte()
        scalar_stats = {}
        loss = 0
        ## 下面使用的net_utils.sigmoid，经过sigmoid后，将最小值设定为了1e-4,最大值设定为了1-1e-4
        mask_loss = self.m_crit(net_utils.sigmoid(output['mask']), batch['cmask'])  ## 预测的boundary mask loss ,用的focal loss
        scalar_stats.update({'mask_loss': mask_loss})
        loss += mask_loss
        #loss += mask_loss2
        ct_loss = self.ct_crit(net_utils.sigmoid(output['ct_hm']), batch['ct_hm'])  ##直接计算ct_hm的loss，这样对所有点都计算了个loss，这里因为加了一个高斯模糊核，在左上和右下一定范围内都一定程度减少了损失
        scalar_stats.update({'ct_loss': ct_loss})
        loss += ct_loss
        #loss += ct_loss2
        wh_loss = self.py_crit(output['poly_init'], output['i_gt_py'])  ## 初始点的点坐标loss使用smooth-l1 loss，与目标检测一样
        vis_wh_loss = self.py_crit(output['vis_poly_init'], output['vis_i_gt_py']) ## visible wh loss
        #wh_loss2 = self.py_crit(mutli_feat_output['poly_init2'], output['i_gt_py'])
        scalar_stats.update({'wh_loss': 0.1 * wh_loss,'vis_wh_loss': 0.1 * vis_wh_loss})  ## 降低了这部分loss的梯度
        loss += 0.1 * wh_loss
        loss += 0.1 * vis_wh_loss
        n_predictions = len(output['py_pred'])  ## 将每次迭代的预测点都计算一次loss
        py_loss = 0.0
        shape_loss = 0.0
        #poly_classify_losses = 0
        #visible_mask = output['per_vis_cmask'][:,None,:,:]
        #gt_poly_classify = get_gcn_feature(visible_mask, output['poly_init']/snake_config.ro ,output['ind'], visible_mask.size(2), visible_mask.size(3))
        #poly_classify_loss = net_utils.dice_coefficient(net_utils.sigmoid(output['poly_pred'][0]), gt_poly_classify)
        #poly_classify_losses += poly_classify_loss.mean()
        
        #cls_loss = 0.0
        py_dis = torch.cat((output['i_gt_py'][:,1:], output['i_gt_py'][:,0].unsqueeze(1)), dim=1)  ## 将i_gt_py整体循环左移方便后面求距离
        tar_shape = py_dis - output['i_gt_py']  ##得出gt点的相对的偏移量
        
        vis_py_dis = torch.cat((output['vis_i_gt_py'][:,1:], output['vis_i_gt_py'][:,0].unsqueeze(1)), dim=1)
        vis_tar_shape = vis_py_dis - output['vis_i_gt_py']
        for i in range(n_predictions):
            i_weight = 0.8**(n_predictions - i - 1)  ### 这个权重越靠近后预测的权重越大,越靠后的应该权重越大，预测错误的话惩罚应该给的更大
            py_loss += i_weight * self.py_crit(output['py_pred'][i], output['i_gt_py'])
            py_loss += i_weight * self.py_crit(output['vis_py_pred'][i], output['vis_i_gt_py'])
            shape_loss += i_weight * self.shape_loss(output['py_pred'][i], tar_shape)
            shape_loss += i_weight * self.shape_loss(output['vis_py_pred'][i], vis_tar_shape)
            #cls_loss += self.cls_crit(output['cls_scores'][i], batch['ct_cls'][ct_01])
            #gt_poly_classify = get_gcn_feature(visible_mask, output['py_pred'][i]/snake_config.ro ,output['ind'], visible_mask.size(2), visible_mask.size(3))
            #poly_classify_loss = net_utils.dice_coefficient(net_utils.sigmoid(output['poly_pred'][i+1]), gt_poly_classify)
            #poly_classify_loss = self.cls_crit(output['poly_pred'][i+1], gt_poly_classify)
            #poly_classify_losses += poly_classify_loss         .mean()
            
        py_loss = py_loss / n_predictions  ## loss均分，与cascade中一样，防止过度训练，而且因为后续迭代的loss也会影响到之前的参数训练
        shape_loss = shape_loss / n_predictions
        #cls_loss = cls_loss / n_predictions
        scalar_stats.update({'py_loss': py_loss})
        scalar_stats.update({'shape_loss': shape_loss})
        #scalar_stats.update({'cls_loss': cls_loss})
        loss += py_loss
        loss += shape_loss
        #loss += cls_loss
        mask_losses = 0
        # if not self.training:
        #     print('debug')
        #pred_masks = self.postprocess(output['mask_preds'], output['per_ins_cmask'].shape[1], output['per_ins_cmask'].shape[2])
        gt_masks_coarse_loss = 0
        bfg_loss = 0
        patch_vector_loss = 0
        
        for i in range(len(output['mask_preds'])):
            pred_masks = output['mask_preds'][i]
            pred_masks=pred_masks.squeeze(1)
            #pred_masks = pred_masks[torch.arange(pred_masks.shape[0]),batch['ct_cls'][batch['ct_01'].byte()]]
            dct_gt_masks,gt_masks_coarse,gt_bfg =self.get_gt_mask(output['per_ins_cmask'], output['rois'][i])
            gt_masks_coarse_loss += self.dct_stage_loss_para[i]*F.l1_loss(output['dct_x'][i],gt_masks_coarse)
            bfg_loss += self.dct_stage_loss_para[i]*F.cross_entropy(output['dct_bfg'][i], gt_bfg)
            
           
            # gt_poly_classify = get_gcn_feature(visible_mask, output['py_pred'][i],output['ind'], visible_mask.size(2), visible_mask.size(3))
            # poly_classify_loss = net_utils.dice_coefficient(net_utils.sigmoid(output['poly_pred'][i]), gt_poly_classify)
            # poly_classify_losses += poly_classify_loss.mean()
            
            patch_vectors = output['dct_patch_vector'][i][gt_bfg == 1, :]
            patch_vector_loss += self.dct_stage_loss_para[i]*F.l1_loss(patch_vectors,dct_gt_masks)
            
            gt_masks = self.crop_and_resize(output['per_ins_cmask'], output['rois'][i]) 
            mask_loss = net_utils.dice_coefficient(net_utils.sigmoid(pred_masks), gt_masks)
            mask_losses +=mask_loss.mean()
        
        for i in range(len(output['vis_mask_preds'])):
            pred_masks = output['vis_mask_preds'][i]
            pred_masks = pred_masks[torch.arange(pred_masks.shape[0]),batch['ct_cls'][batch['ct_01'].byte()]]
            gt_masks = self.vis_crop_and_resize(output['per_vis_cmask'], output['vis_rois'][i])
            mask_loss = net_utils.dice_coefficient(net_utils.sigmoid(pred_masks), gt_masks)
            mask_losses +=mask_loss.mean()
        
        #poly_classify_losses/len(output['mask_preds'])
        scalar_stats.update({'dct_x_loss': gt_masks_coarse_loss,'dct_bfg_loss':bfg_loss,'dct_patch_vector_loss':patch_vector_loss})
        mask_losses = mask_losses / len(output['mask_preds'])
        mask_losses = mask_losses + gt_masks_coarse_loss + bfg_loss + patch_vector_loss
        scalar_stats.update({'box_mask_loss': mask_losses})
        loss += mask_losses
        #loss += poly_classify_losses
        scalar_stats.update({'loss': loss})
        image_stats = {}

        return output, loss, scalar_stats, image_stats